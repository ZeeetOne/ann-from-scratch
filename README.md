# ğŸ§  ANN from Scratch v2.0 - Professional Neural Network Framework

[![Version](https://img.shields.io/badge/version-2.0.0-blue.svg)](https://github.com/yourrepo/ann-from-scratch)
[![Python](https://img.shields.io/badge/python-3.8+-green.svg)](https://python.org)
[![License](https://img.shields.io/badge/license-MIT-orange.svg)](LICENSE)

**Interactive web application for building, training, and testing Artificial Neural Networks from scratch using Python and NumPy.**

> ğŸ‰ **v2.0 Major Refactoring**: Completely rewritten with professional software engineering practices! Clean Architecture, SOLID principles, comprehensive testing, and full documentation.

## âœ¨ What's New in v2.0

### ğŸ—ï¸ Professional Architecture
- **Clean Architecture** with clear separation of concerns
- **SOLID Principles** applied throughout the codebase
- **Design Patterns**: Strategy, Factory, Facade, Application Factory
- **Modular Structure**: Backend (Core/Services/API), Frontend, Tests, Docs
- **Scalable & Maintainable**: Easy to understand, extend, and test

### ğŸ§ª Comprehensive Testing
- **Unit Tests**: All core components tested in isolation
- **Integration Tests**: 100 comprehensive test cases covering complete web workflow
- **100% Pass Rate**: All 100 tests passed (1.552 seconds execution time)
- **Complete Coverage**: Network building, dataset generation, forward pass, loss calculation, backpropagation, training
- **Manual Verification**: Web results match manual calculations with high precision
- **Architecture Coverage**: 30 different architectures from minimal (2-2-1) to complex (6-12-8-6-3)
- **All Features Tested**: All web interface functions integrated with ANN functionality

### ğŸ“š Full Documentation
- **[Architecture Documentation](docs/ARCHITECTURE.md)**: Design decisions and patterns
- **[API Documentation](docs/API.md)**: Complete REST API reference
- **[Training Guide](docs/TRAINING_GUIDE.md)**: Hyperparameter tuning best practices

### âš¡ Enhanced Features
- **Factory Pattern** for easy extension (add new activations, losses, optimizers)
- **Better Error Handling** with centralized middleware
- **Request Validation** with clear error messages
- **Dependency Injection** for testability
- **Configuration Management** for different environments

## ğŸ¯ Features

### Network Configuration
- **Flexible Architecture**: Define custom layer sizes and connections
- **Multiple Activation Functions**:
  - Sigmoid, ReLU, Linear, Softmax, Threshold
- **Custom Connections**: Manually specify node connections
- **Smart Weight Initialization**: Xavier/He initialization for optimal training
  - Xavier initialization for sigmoid/tanh (prevents vanishing gradients)
  - He initialization for ReLU (prevents dying neurons)
- **Random Dataset Generation**: Automatically generate datasets matching network architecture

### Training & Optimization
- **Backpropagation**: Complete implementation from scratch
- **Multiple Optimizers**:
  - **Gradient Descent (GD)**: Full batch, stable convergence
  - **Stochastic Gradient Descent (SGD)**: Mini-batch support
  - **Momentum**: Accelerated convergence (new!)
- **Multiple Loss Functions**:
  - Mean Squared Error (MSE)
  - Binary Cross-Entropy
  - Categorical Cross-Entropy
- **Hyperparameter Control**: Learning rate, epochs, batch size

### Evaluation & Visualization
- **Comprehensive Metrics**: Accuracy, precision, recall, F1-score
- **Confusion Matrix**: Per-class performance analysis
- **Training History**: Loss curve visualization
- **Detailed Forward Pass**: Layer-by-layer activation inspection

### User Interface
- **Interactive Web UI**: Modern, user-friendly interface
- **Real-time Results**: Instant feedback during training
- **Responsive Design**: Works on desktop and mobile
- **API Access**: Full REST API for programmatic use

## ğŸ“¦ Installation

### Prerequisites
- Python 3.8+
- pip package manager

### Steps

1. **Clone or Download Repository**
   ```bash
   git clone https://github.com/yourrepo/ann-from-scratch.git
   cd ann-from-scratch
   ```

2. **Create Virtual Environment** (Recommended)
   ```bash
   python -m venv venv

   # Windows
   venv\Scripts\activate

   # Linux/Mac
   source venv/bin/activate
   ```

3. **Install Dependencies**
   ```bash
   pip install -r requirements.txt
   ```

## ğŸš€ Quick Start

### Option 1: Web Interface

1. **Run the application:**
   ```bash
   python run.py
   ```

2. **Open browser:**
   ```
   http://localhost:5000
   ```

3. **Try examples:**
   - Click "Load Example Network" for quick start
   - Modify parameters and train
   - See results instantly

### Option 2: Python API

```python
from backend.core import NeuralNetwork
import numpy as np

# Build network (2-2-1 for AND gate)
network = NeuralNetwork()
network.add_layer(2, 'linear')    # Input
network.add_layer(2, 'sigmoid')   # Hidden
network.add_layer(1, 'sigmoid')   # Output

# Set connections
network.set_connections(
    1, [[0,1], [0,1]],
    [[0.5,-0.3], [-0.4,0.6]],
    [0.1,-0.2]
)
network.set_connections(
    2, [[0,1]],
    [[0.8,-0.5]],
    [0.2]
)

# Prepare AND gate data
X = np.array([[0,0], [0,1], [1,0], [1,1]])
y = np.array([[0], [0], [0], [1]])

# Train
history = network.train(
    X, y,
    epochs=500,
    learning_rate=0.5,
    optimizer='gd',
    loss_function='mse'
)

# Predict
y_pred_classes, y_pred_probs = network.predict(X)
print(f"Final Loss: {history['loss'][-1]:.6f}")
print(f"Predictions: {y_pred_probs.flatten()}")
```

### Option 3: REST API

```python
import requests

# Quick start with example network
response = requests.post('http://localhost:5000/quick_start_binary')
data = response.json()
dataset = data['example_dataset']

# Train the network
train_response = requests.post('http://localhost:5000/train', json={
    'dataset': dataset,
    'epochs': 1000,
    'learning_rate': 0.5,
    'optimizer': 'gd',
    'loss_function': 'binary'
})

result = train_response.json()
print(f"Accuracy: {result['accuracy'] * 100:.2f}%")
```

### Option 4: Custom Network with Random Dataset

```python
import requests

# Build custom network (3-5-1 binary classifier)
build_response = requests.post('http://localhost:5000/build_network', json={
    'layers': [
        {'num_nodes': 3, 'activation': 'linear'},
        {'num_nodes': 5, 'activation': 'sigmoid'},
        {'num_nodes': 1, 'activation': 'sigmoid'}
    ]
})

# Generate random dataset matching the network
dataset_response = requests.post('http://localhost:5000/generate_random_dataset', json={
    'num_samples': 20
})
dataset = dataset_response.json()['dataset']

# Train with random dataset
train_response = requests.post('http://localhost:5000/train', json={
    'dataset': dataset,
    'epochs': 100,
    'learning_rate': 0.3,
    'optimizer': 'gd',
    'loss_function': 'binary'
})

print(f"Training complete: {train_response.json()['accuracy'] * 100:.2f}%")
```

## ğŸ“ Project Structure

```
ann-from-scratch/
â”œâ”€â”€ backend/                    # Backend Application
â”‚   â”œâ”€â”€ core/                   # Core ML Algorithms
â”‚   â”‚   â”œâ”€â”€ activation_functions.py  # Sigmoid, ReLU, Softmax, etc.
â”‚   â”‚   â”œâ”€â”€ loss_functions.py        # MSE, Cross-Entropy, etc.
â”‚   â”‚   â”œâ”€â”€ optimizers.py            # GD, SGD, Momentum
â”‚   â”‚   â””â”€â”€ neural_network.py        # Main Network Class
â”‚   â”œâ”€â”€ services/               # Business Logic
â”‚   â”‚   â”œâ”€â”€ network_service.py       # Network management
â”‚   â”‚   â”œâ”€â”€ training_service.py      # Training orchestration
â”‚   â”‚   â””â”€â”€ data_service.py          # Data processing
â”‚   â”œâ”€â”€ api/                    # REST API
â”‚   â”‚   â”œâ”€â”€ routes/             # Endpoint definitions
â”‚   â”‚   â”œâ”€â”€ middleware/         # Error handling
â”‚   â”‚   â””â”€â”€ app.py              # Flask application
â”‚   â”œâ”€â”€ utils/                  # Utilities
â”‚   â””â”€â”€ config.py               # Configuration
â”œâ”€â”€ frontend/                   # Frontend Application
â”‚   â”œâ”€â”€ static/                 # CSS & JavaScript
â”‚   â””â”€â”€ templates/              # HTML templates
â”œâ”€â”€ tests/                      # Test Suite
â”‚   â”œâ”€â”€ unit/                   # Unit tests
â”‚   â””â”€â”€ integration/            # Integration tests
â”œâ”€â”€ examples/                   # Usage examples
â”œâ”€â”€ docs/                       # Documentation
â”‚   â”œâ”€â”€ test-reports/           # Test execution reports
â”‚   â”œâ”€â”€ examples/               # Code walkthroughs & tutorials
â”‚   â”œâ”€â”€ ARCHITECTURE.md         # Design & patterns
â”‚   â”œâ”€â”€ API.md                  # API reference
â”‚   â””â”€â”€ TRAINING_GUIDE.md       # Training tips
â”œâ”€â”€ requirements.txt            # Dependencies
â””â”€â”€ run.py                      # Application entry point
```

## ğŸ“ Usage Examples

### Binary Classification

```python
# Load example binary classification network (3-4-1)
response = requests.post('http://localhost:5000/quick_start_binary')
# Returns: Student pass/fail prediction network
```

### Multi-Class Classification

```python
# Load example multi-class network (3-4-2 with softmax)
response = requests.post('http://localhost:5000/quick_start_multiclass')
# Returns: Weather prediction network (rain/sunny)
```

### Custom Network

```python
# Build your own network
network = NeuralNetwork()
network.add_layer(3, 'linear')      # 3 input features
network.add_layer(4, 'sigmoid')     # 4 hidden neurons
network.add_layer(2, 'softmax')     # 2 classes

# Fully connected layers
network.set_full_connections(
    1,
    weight_matrix=np.random.randn(4, 3) * 0.5,
    biases=np.zeros(4)
)
network.set_full_connections(
    2,
    weight_matrix=np.random.randn(2, 4) * 0.5,
    biases=np.zeros(2)
)

# Train with your data
history = network.train(X_train, y_train, epochs=1000)
```

## ğŸ§ª Testing

### Run Complete Integration Test Suite (100 tests) â­ RECOMMENDED
```bash
python -m unittest tests.integration.test_100_complete_integration
```
**This is the main comprehensive test suite covering all web interface functions integrated with ANN functionality.**

### Run Additional Test Suites
```bash
# 50 comprehensive test cases
python -m unittest tests.integration.test_50_comprehensive_cases

# 20 network configuration tests
python -m unittest tests.integration.test_20_network_configurations
```

### Run Unit Tests
```bash
python tests/unit/test_activation_functions.py
```

### Run Specific Integration Tests
```bash
python tests/integration/test_complete_workflow.py
python tests/integration/test_manual_verification.py
python tests/integration/test_random_datasets.py
```

### Run API Tests
```bash
# First, start the server:
python run.py

# In another terminal:
python tests/integration/test_web_api.py
```

### Test Coverage

**100 Complete Integration Tests:**
- âœ… Network Building (20 tests): Various architectures and activations
- âœ… Dataset Generation (20 tests): Random data matching network architecture
- âœ… Forward Pass (20 tests): All activation functions validated
- âœ… Loss Calculation (10 tests): All loss functions verified
- âœ… Backpropagation (10 tests): Gradient computation validated
- âœ… Training (15 tests): All optimizers tested (GD, SGD, Momentum)
- âœ… Manual Verification (5 tests): Results match manual calculations

**Additional Coverage:**
- **50 Comprehensive Cases**: Complete web workflow from network building to training
- **20 Network Configurations**: Various architectures and activation functions
- **Manual Verification**: Every operation verified against manual calculations

## ğŸ“Š Performance

### 100 Complete Integration Tests
- **Test Success Rate**: 100% (100/100 tests passed)
- **Execution Time**: 1.552 seconds for all 100 tests
- **Average per Test**: ~0.015 seconds
- **Network Architectures**: 30 different architectures tested
- **Classification Types**: Binary, Multi-class (3-10 classes), Multi-label (2-5 labels)
- **Activations Tested**: Sigmoid, ReLU, Softmax, Linear, Mixed
- **Optimizers Tested**: GD, SGD, Momentum (all passed)
- **Loss Functions**: Binary Cross-Entropy, Categorical Cross-Entropy, MSE

### Training Performance (from 50 comprehensive test cases)
- **Binary Classification**: 53% to 100% accuracy depending on complexity
- **Multi-Class Classification**: 48% to 85% accuracy for 3-10 classes
- **Multi-Label Classification**: 79% to 94% accuracy for 2-5 labels
- **Loss Reduction**: Average 15-20% improvement per training session
- **Training Speed**: Efficient handling of datasets up to 100 samples
- **Architecture Range**: From minimal (2-2-1) to complex (7-15-10-8-5-3)

Key Achievements:
- Fixed probability stuck at 0.300 issue with proper weight initialization
- Deep networks (6+ layers) work correctly with ReLU activation
- Manual calculations match network calculations with <1e-6 precision

## ğŸ”§ Configuration

### Environment Variables

```bash
# Development mode
DEBUG=True
HOST=localhost
PORT=5000

# Production mode
DEBUG=False
HOST=0.0.0.0
PORT=80
```

### Configuration Files

```python
# Use different configs
python run.py --config development  # Default
python run.py --config production
python run.py --config testing
```

## ğŸ“š Documentation

### Core Documentation
- **[Architecture Guide](docs/ARCHITECTURE.md)** - Design decisions, patterns, and extension points
- **[API Reference](docs/API.md)** - Complete REST API documentation
- **[Training Guide](docs/TRAINING_GUIDE.md)** - Best practices for training networks

### Test Reports
- **[100 Complete Integration Test Report](docs/test-reports/100-complete-integration-test-report.md)** â­ - 100 complete tests covering all web functions
- **[Comprehensive Test Report](docs/test-reports/comprehensive-test-report.md)** - 50 comprehensive test cases
- **[Network Configurations Test Report](docs/test-reports/network-configurations-test-report.md)** - 20 network architecture tests

### Examples & Walkthroughs
- **[Binary Classification Walkthrough](docs/examples/binary-classification-walkthrough.md)** - Step-by-step manual calculations
- **[Code Examples](examples/)** - Python examples for common tasks

## ğŸ› ï¸ Technologies

- **Backend**: Python 3.8+, Flask
- **Computation**: NumPy
- **Data Processing**: Pandas
- **Frontend**: HTML5, CSS3, JavaScript (ES6+)
- **Architecture**: Clean Architecture, SOLID Principles
- **Patterns**: Strategy, Factory, Facade, Application Factory

## ğŸ¯ Key Design Patterns

### Strategy Pattern
```python
# Easy to swap algorithms
network.train(X, y, optimizer='sgd')   # Use SGD
network.train(X, y, optimizer='gd')    # Use GD
network.train(X, y, optimizer='momentum')  # Use Momentum
```

### Factory Pattern
```python
# Create instances by name
activation = ActivationFactory.create('sigmoid')
loss = LossFunctionFactory.create('binary')
optimizer = OptimizerFactory.create('sgd', learning_rate=0.01)
```

### Facade Pattern
```python
# Services simplify complex operations
results = TrainingService.train_network(network, X, y, config)
predictions = DataService.process_predictions(network, X, y)
```

## ğŸš€ Extending the Framework

### Add New Activation Function

```python
from backend.core import ActivationFunction, ActivationFactory

class Tanh(ActivationFunction):
    def forward(self, x):
        return np.tanh(x)

    def derivative(self, x):
        return 1 - np.tanh(x)**2

    @property
    def name(self):
        return "tanh"

# Register
ActivationFactory.register('tanh', Tanh)

# Use
network.add_layer(4, 'tanh')
```

### Add New Optimizer

```python
from backend.core import Optimizer, OptimizerFactory

class Adam(Optimizer):
    def __init__(self, learning_rate=0.001, beta1=0.9, beta2=0.999):
        super().__init__(learning_rate)
        self.beta1 = beta1
        self.beta2 = beta2
        # ... initialize momentum and velocity

    def update(self, params, gradients):
        # ... Adam update logic
        return updated_params

    @property
    def name(self):
        return "adam"

# Register
OptimizerFactory.register('adam', Adam)

# Use
network.train(X, y, optimizer='adam')
```

## ğŸ¤ Contributing

We welcome contributions! Areas for improvement:

- [ ] Adam optimizer
- [ ] Batch normalization
- [ ] Dropout regularization
- [ ] L1/L2 regularization
- [ ] Learning rate scheduling
- [ ] Early stopping
- [ ] Model save/load
- [ ] More activation functions (tanh, leaky ReLU, ELU)
- [ ] Visualization improvements
- [ ] Performance optimizations

## ğŸ“ License

MIT License - Free to use for educational and commercial purposes.

## ğŸ™ Acknowledgments

Built with best practices from:

- **Martin Fowler** - Refactoring
- **Robert C. Martin** - Clean Architecture, SOLID Principles
- **Gang of Four** - Design Patterns
- **Miguel Grinberg** - Flask Web Development

## ğŸ“§ Support

- **Issues**: Create an issue on GitHub
- **Documentation**: See [docs/](docs/) directory
- **Examples**: See [examples/](examples/) directory

## ğŸŒŸ Star History

If you find this project useful, please consider giving it a star â­

---

**Built with â¤ï¸ for Deep Learning Education**

*Learn by building. Understand by implementing. Master by teaching.*

**Version 2.0.0** - Professional Edition with Clean Architecture
